{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-3-8386af9e8ea8>, line 54)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-8386af9e8ea8>\"\u001b[0;36m, line \u001b[0;32m54\u001b[0m\n\u001b[0;31m    idxs = np.random.choice((num_train,batch_size))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from cs231n.classifiers.linear_svm import *\n",
    "from cs231n.classifiers.softmax import *\n",
    "\n",
    "class LinearClassifier(object):\n",
    "\n",
    "  def __init__(self):\n",
    "    self.W = None\n",
    "\n",
    "  def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "    \"\"\"\n",
    "    Train this linear classifier using stochastic gradient descent.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "      training samples each of dimension D.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
    "      means that X[i] has label 0 <= c < C for C classes.\n",
    "    - learning_rate: (float) learning rate for optimization.\n",
    "    - reg: (float) regularization strength.\n",
    "    - num_iters: (integer) number of steps to take when optimizing\n",
    "    - batch_size: (integer) number of training examples to use at each step.\n",
    "    - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "    Outputs:\n",
    "    A list containing the value of the loss function at each training iteration.\n",
    "    \"\"\"\n",
    "    num_train, dim = X.shape\n",
    "    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
    "    if self.W is None:\n",
    "      # lazily initialize W\n",
    "      self.W = 0.001 * np.random.randn(dim, num_classes)\n",
    "\n",
    "    # Run stochastic gradient descent to optimize W\n",
    "    loss_history = []\n",
    "    for it in range(num_iters):\n",
    "      X_batch = None\n",
    "      y_batch = None\n",
    "\n",
    "      #########################################################################\n",
    "      # TODO:                                                                 #\n",
    "      # Sample batch_size elements from the training data and their           #\n",
    "      # corresponding labels to use in this round of gradient descent.        #\n",
    "      # Store the data in X_batch and their corresponding labels in           #\n",
    "      # y_batch; after sampling X_batch should have shape (dim, batch_size)   #\n",
    "      # and y_batch should have shape (batch_size,)                           #\n",
    "      #                                                                       #\n",
    "      # Hint: Use np.random.choice to generate indices. Sampling with         #\n",
    "      # replacement is faster than sampling without replacement.              #\n",
    "      #########################################################################\n",
    "        idxs = np.random.choice((num_train,batch_size))\n",
    "        X_batch = self.X_train[idxs,:]\n",
    "        Y_batch = self.Y_train[idxs]\n",
    "      #######################################################################\n",
    "      #                       END OF YOUR CODE                                #\n",
    "      #########################################################################\n",
    "\n",
    "      # evaluate loss and gradient\n",
    "      loss, grad = self.loss(X_batch, y_batch, reg)\n",
    "      loss_history.append(loss)\n",
    "\n",
    "      # perform parameter update\n",
    "      #########################################################################\n",
    "      # TODO:                                                                 #\n",
    "      # Update the weights using the gradient and the learning rate.          #\n",
    "      #########################################################################\n",
    "      self.W -= learning_rate*grad\n",
    "      \n",
    "      #########################################################################\n",
    "      #                       END OF YOUR CODE                                #\n",
    "      #########################################################################\n",
    "\n",
    "      if verbose and it % 100 == 0:\n",
    "        print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "    return loss_history\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"\n",
    "    Use the trained weights of this linear classifier to predict labels for\n",
    "    data points.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "      training samples each of dimension D.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
    "      array of length N, and each element is an integer giving the predicted\n",
    "      class.\n",
    "    \"\"\"\n",
    "    y_pred = np.zeros(X.shape[0])\n",
    "    ###########################################################################\n",
    "    # TODO:                                                                   #\n",
    "    # Implement this method. Store the predicted labels in y_pred.            #\n",
    "    ###########################################################################\n",
    "    score = X.dot(self.W)\n",
    "    y_pred = np.argmax(score,axis=1)\n",
    "    ###########################################################################\n",
    "    #                           END OF YOUR CODE                              #\n",
    "    ###########################################################################\n",
    "    return y_pred\n",
    "  \n",
    "  def loss(self, X_batch, y_batch, reg):\n",
    "    \"\"\"\n",
    "    Compute the loss function and its derivative. \n",
    "    Subclasses will override this.\n",
    "\n",
    "    Inputs:\n",
    "    - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n",
    "      data points; each point has dimension D.\n",
    "    - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
    "    - reg: (float) regularization strength.\n",
    "    Returns: A tuple containing:\n",
    "    - loss as a single float\n",
    "    - gradient with respect to self.W; an array of the same shape as W\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "class LinearSVM(LinearClassifier):\n",
    "  \"\"\" A subclass that uses the Multiclass SVM loss function \"\"\"\n",
    "\n",
    "  def loss(self, X_batch, y_batch, reg):\n",
    "    return svm_loss_vectorized(self.W, X_batch, y_batch, reg)\n",
    "\n",
    "\n",
    "class Softmax(LinearClassifier):\n",
    "  \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n",
    "\n",
    "  def loss(self, X_batch, y_batch, reg):\n",
    "    return softmax_loss_vectorized(self.W, X_batch, y_batch, reg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
