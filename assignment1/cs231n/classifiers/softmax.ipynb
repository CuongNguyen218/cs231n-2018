{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss_naive(W, X, y, reg):\n",
    "  \"\"\"\n",
    "  Softmax loss function, naive implementation (with loops)\n",
    "\n",
    "  Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "  of N examples.\n",
    "\n",
    "  Inputs:\n",
    "  - W: A numpy array of shape (D, C) containing weights.\n",
    "  - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "  - y: A numpy array of shape (N,) containing training labels; \n",
    "  - y[i] = c means that X[i] has label c, where 0 <= c < C.\n",
    "  - reg: (float) regularization strength\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - loss as single float\n",
    "  - gradient with respect to weights W; an array of same shape as W\n",
    "  \"\"\"\n",
    "  # Initialize the loss and gradient to zero.\n",
    "  loss = 0.0\n",
    "  dW = np.zeros_like(W)\n",
    "\n",
    "  #############################################################################\n",
    "  # TODO: Compute the softmax loss and its gradient using explicit loops.     #\n",
    "  # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
    "  # here, it is easy to run into numeric instability. Don't forget the        #\n",
    "  # regularization!                                                           #\n",
    "  #############################################################################\n",
    "  num_train = X.shape[0]\n",
    "  for i in range num_train:\n",
    "    scores = np.exp(X[i].dot(W)) # e ^ W.X cai nay la tung phan tu N x C\n",
    "    scores /= np.sum(scores) # normalization\n",
    "    # y[i] = c \n",
    "    loss -= np.log(scores[y[i]]) # tinh ham loss func L = -tong(log(score))\n",
    "    scores[y[i]] -= 1 # ?????? y[y] \n",
    "    dW += X[i].T.reshape(-1, 1).dot(scores.T.reshape(1, -1))\n",
    "  loss /= num_train\n",
    "  loss += 0.5*reg*np.sum(W*W)\n",
    "  dW /= num_train ## tinh dao ham theo cong thuc 15.13 trong sach \n",
    "  #############################################################################\n",
    "  #                          END OF YOUR CODE                                 #\n",
    "  #############################################################################\n",
    "\n",
    "  return loss, dW\n",
    "\n",
    "\n",
    "def softmax_loss_vectorized(W, X, y, reg):\n",
    "  \"\"\"\n",
    "  Softmax loss function, vectorized version.\n",
    "\n",
    "  Inputs and outputs are the same as softmax_loss_naive.\n",
    "  \"\"\"\n",
    "  # Initialize the loss and gradient to zero.\n",
    "  loss = 0.0\n",
    "  dW = np.zeros_like(W)\n",
    "\n",
    "  #############################################################################\n",
    "  # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
    "  # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
    "  # here, it is easy to run into numeric instability. Don't forget the        #\n",
    "  # regularization!                                                           #\n",
    "  #############################################################################\n",
    "  num_train = X.shape[0]\n",
    "  Z = X.dot(W)\n",
    "  e_Z = np.exp(Z - np.max(Z,axis=1,keepdims=True))\n",
    "  S = e_Z / e_Z.sum(axis = 1,keepdims= True)\n",
    "  \n",
    "  C = S[np.arange(S.shape[0]),y]\n",
    "  loss -= np.sum(np.log(C))\n",
    "  loss /= num_train\n",
    "  loss += 0.5*reg*np.sum(W*W)\n",
    "\n",
    "  S[np.arange(S.shape[0]),y] -= 1 #?????\n",
    "  dW += X.T.dot(S)\n",
    "  dW /= num_train\n",
    "  dW += reg*W\n",
    "  #############################################################################\n",
    "  #                          END OF YOUR CODE                                 #\n",
    "  #############################################################################\n",
    "\n",
    "  return loss, dW\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
